- Time：2023-06-04 19:13
- Label： #spider #python

## Abstract

总结爬虫实战内容

- [x] 抓取网吧电影 ✅ 2023-06-06
- [ ] X 视频
- [ ] X 站
- [ ] X 头条
- [ ] 得物 APP
- [ ] B 站
- [ ] 抖音
- [ ] 知乎

## Content

### 抓取网吧电影

要把视频切成非常多个小碎片. 那就需要有个文件来记录这些小碎片的路径. 该文件一般为 M3U 文件. M3U 文件中的内容经过 UTF-8 的编码后, 就是 M3U8 文件. 今天, 我们看到的各大视频网站平台使用的几乎都是 M3U8 文件.

![[Snipaste/Pasted image 20230606162002.png]]

先做一个小总结, 针对网吧电影这个网站而言, 我们想要得到 M3U8 文件, 过程是:

1. 访问视频主页的页面源代码. 提取到 iframe 中的 src 属性. 暂时记做 iframe_url
2. 提取 iframe_url 的页面源代码. 提取到 M3U8 文件的地址

> 注意, 有些网站不是这样的结构, 所以该逻辑未必适用于所有视频网站. 不过一般的网站都大同小异, 只要盯着 M3U8 即可

简单解读一下. 前面的内容, 只有 EXT-X-KEY 有用. 这里面的 METHOD=AES-128 表示该视频是经过加密的. 需要进行解密. 解密的秘钥存放在 URI 对应的网址内.

后面就简单多了. 每一个不以#开头的, 都是视频切片 ts 文件.

后期解析起来的整体思路也就出来了.

1. 下载每一个 ts 文件
2. 获取到秘钥 key
3. 使用 AES 对 ts 文件进行解密.
4. 将众多 ts 文件合并为 MP4 文件.  

![[Snipaste/Pasted image 20230606163729.png]]  

文件中列的顺序就是需要合并的顺序

#### 解决方案 1

```python
"""
1. 一般情况下. 一个网页里想要显示出一个视频资源. <video>
2. 几乎没有视频网站会在video中直接给出视频的地址
    #                                  1G 2G 4G
    # <video src="http://www.baidu.com/苍老师.MP4"></video>
    # 用户体验极差. 占网速, 占内存
    # 切片
    =  abc.ts
    =  efg.ts
    =  123.ts
    # 必须把ts文件的正确顺序进行保存 -> M3U文件 -> utf-8 -> M3U8文件
3. 正确的视频加载过程:
    1. 先请求到M3U8文件
    2. 加载ts文件
    3. 正常播放视频资源
    服务器压力小. 用户体验好.
"""

# 针对网吧电影的分析:
# 1. 拿到视频页的页面源代码.
# 2. 从视频页的页面源代码中找到对应的iframe, 提取到iframe里面的src
# 3. 请求到src对应的页面源代码. 在该页面中解析出真正的M3U8文件地址
# 4. 下载第一层M3U8. 从第一层M3U8中解析出第二层的地址
# 5. 下载第二层M3U8. 从第二层M3U8中解析出每一个TS文件的路径, 启动协程任务
# 6. 对ts文件进行解密操作: 先拿key
# 7. 对ts文件进行合并. 还原回mp4文件

import requests
from lxml import etree
from urllib import parse
import re
import asyncio
import aiohttp
import aiofiles
from Crypto.Cipher import AES  # pip install pycryptodome
import os  # 用它来执行操作系统的命令


def get_page_source(url):
    resp = requests.get(url)
    return resp.text


def get_iframe_src(url):
    print("获取iframe的src值")
    # 1. 拿到视频页的页面源代码.
    page_source = get_page_source(url)
    # 2. 从视频页的页面源代码中找到对应的iframe, 提取到iframe里面的src
    tree = etree.HTML(page_source)
    src = tree.xpath("//iframe/@src")[0]
    # /js/player/?url=https://video.buycar5.cn/20200824/1EcQQ5Ww/index.m3u8&id=30288&num=1&count=1&vt=1
    src_url = parse.urljoin(url, src)
    print("成功获取iframe的src值", src_url)
    return src_url


def get_first_m3u8_url(src_url):
    print("获取第一层M3U8地址")
    page_source = get_page_source(src_url)
    # 在js里提取数据. 最好用的方案: re
    obj = re.compile(r'url: "(?P<m3u8_url>.*?)",', re.S)
    result = obj.search(page_source)
    m3u8_url = result.group("m3u8_url")
    print("成功获取到第一层M3U8地址")
    return m3u8_url


def download_m3u8_file(first_m3u8_url):
    print("下载第二层M3U8地址")
    # 获取到第二层M3U8文件, 并保存在硬盘中
    first_m3u8 = get_page_source(first_m3u8_url)
    second_m3u8_url = first_m3u8.split()[-1]
    second_m3u8_url = parse.urljoin(first_m3u8_url, second_m3u8_url)

    # 下载第二层M3U8文件
    second_m3u8 = get_page_source(second_m3u8_url)
    with open("second_m3u8.txt", mode='w', encoding='utf-8') as f:
        f.write(second_m3u8)
    print("下载第二层M3U8成功....数据以保存在second_m3u8.txt 文件中....")


async def download_one(url):  # url: ts文件的下载路径
    # 自省
    for i in range(10):
        try:
            file_name = url.split("/")[-1]
            async with aiohttp.ClientSession() as session:  # requests, requests.session()
                async with session.get(url, timeout=90) as resp:
                    content = await resp.content.read()
                    async with aiofiles.open(f"./电影_源_加密后/{file_name}", mode="wb") as f:
                        await f.write(content)
            print(url, "下载成功")
            break
        except:
            print("下载失败, 出现错误", url)
            # time.sleep() # 可以适当的进行睡眠
            # 异步爬虫没必要
            await asyncio.sleep((i+1) * 5)


async def download_all_ts():
    tasks = []
    with open("second_m3u8.txt", mode="r", encoding='utf-8') as f:
        for line in f:
            if line.startswith("#"):
                continue
            line = line.strip()  # 必须处理
            task = asyncio.create_task(download_one(line))
            tasks.append(task)
    await asyncio.wait(tasks)


def get_key():
    obj = re.compile(r'URI="(?P<key_url>.*?)"')
    key_url = ""
    with open("second_m3u8.txt", mode="r", encoding='utf-8') as f:
        result = obj.search(f.read())
        key_url = result.group("key_url")
    # 请求到key的url, 获取到真正的秘钥
    key_str = get_page_source(key_url)
    return key_str.encode('utf-8')


async def des_one(file, key):
    print("即将开始解密", file)
    # 加密解密对象创建
    aes = AES.new(key=key, IV=b"0000000000000000", mode=AES.MODE_CBC)
    async with aiofiles.open(f"./电影_源_加密后/{file}", mode="rb") as f1, \
            aiofiles.open(f"./电影_源_解密后/{file}", mode="wb") as f2:
        # 从加密后的文件中读取出来. 进行解密. 保存在未加密文件中
        content = await f1.read()
        bs = aes.decrypt(content)
        await f2.write(bs)
    print("文件已经解密", file)


async def des_all_ts_file(key):
    tasks = []
    with open("second_m3u8.txt", mode="r", encoding='utf-8') as f:
        for line in f:
            if line.startswith("#"):
                continue
            line = line.strip()
            file_name = line.split("/")[-1]
            # 准备异步操作
            task = asyncio.create_task(des_one(file_name, key))
            tasks.append(task)

    await asyncio.wait(tasks)


def merge_ts():
    name_list = []
    with open("second_m3u8.txt", mode="r", encoding='utf-8') as f:
        for line in f:
            if line.startswith("#"):
                continue
            line = line.strip()
            file_name = line.split("/")[-1]
            name_list.append(file_name)
    # print(name_list)
    # 切换工作目录 到 ./电影_源_解密后/
    # 1.记录当前工作目录
    now_dir = os.getcwd()
    print(now_dir)
    # 2. 切换工作目录 到 ./电影_源_解密后/
    os.chdir("./电影_源_解密后/")
    # 分而治之
    # 一次性合并100个文件
    temp = []
    n = 1
    for i in range(len(name_list)):
        name = name_list[i]
        temp.append(name)  # [a.ts, b.ts, c.ts]
        if i != 0 and i % 100 == 0:  # 每100个合并一次
            # 合并,
            # cat a.ts b.ts c.ts > xxx.mp4
            # copy /b a.ts + b.ts + c.ts xxx.mp4
            names = " ".join(temp)
            os.system(f"cat {names} > {n}.ts")
            n += 1
            temp = []  # 还原成新的待合并列表
    # 把最后没有合并的进行收尾
    names = " ".join(temp)
    os.system(f"cat {names} > {n}.ts")
    n += 1

    temp_2 = []
    # 把所有的n进行循环
    for i in range(1, n):
        temp_2.append(f"{i}.ts")

    names = " ".join(temp_2)
    os.system(f"cat {names} > movie.mp4")

    # 3. 所有的操作之后. 一定要把工作目录切换回来
    os.chdir(now_dir)


def main():
    url = "http://www.wbdy.tv/play/30288_1_1.html"
    src_url = get_iframe_src(url)
    # 3.访问src_url. 提取到第一层m3u8文件地址
    first_m3u8_url = get_first_m3u8_url(src_url)
    download_m3u8_file(first_m3u8_url)

    asyncio.run(download_all_ts())

    # loop = asyncio.get_event_loop()
    # loop.run_until_complete(download_all_ts())

    # 进行解密
    key = get_key()
    asyncio.run(des_all_ts_file(key))

    # 合并ts文件
    merge_ts()


if __name__ == '__main__':
    # main()
    # n = 0
    # with open("second_m3u8.txt", mode="r", encoding='utf-8') as f:
    #     for line in f:
    #         if line.startswith("#EXTINF:"):
    #             line = line.strip()
    #             line = line.strip(",")
    #             n += float(line.split(":")[-1])
    n = 7587

    print(n/60)
    print(126.45%60)
```

#### 解决方案 2

```python
import requests
from lxml import etree
import re
from urllib import parse
import asyncio
import aiohttp
import aiofiles
import os
from Crypto.Cipher import AES


def get_page_source(url):
    headers = {
        "Referer": "http://www.wbdy.tv/",
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36"
    }
    resp = requests.get(url, headers=headers)
    return resp.text


def get_iframe_url(url):
    page_source = get_page_source(url)
    tree = etree.HTML(page_source)
    iframe_src = tree.xpath("//iframe[@id='mplay']/@src")[0]
    movie_name = tree.xpath("//h2/text()")[0]
    return movie_name, iframe_src


def get_m3u8_file_url(iframe_url):
    iframe_source = get_page_source(iframe_url)
    obj = re.compile(r'url: "(?P<m3u8>.*?)",')
    result = obj.search(iframe_source)
    m3u8_url = result.group("m3u8")
    return m3u8_url


async def aio_download_ts(save_path, ts_url, session):
    for c in range(10):
        try:
            async with session.get(ts_url) as resp:
                movie_content = await resp.content.read()
                # 存储文件
                async with aiofiles.open(save_path, mode="wb") as f:
                    await f.write(movie_content)
            print(save_path, "下载完毕!~")
            return ""
        except:
            print(ts_url, "下载失败!~, 重新下载. ")
    return ts_url


async def aio_download(movie_name, ts_list):
    tasks = []

    file_path = f"./{movie_name}"
    if not os.path.exists(file_path):
        os.makedirs(file_path)

    async with aiohttp.ClientSession() as session:
        for ts_url in ts_list:
            file_name = ts_url.split("/")[-1]
            movie_save_path = os.path.join(file_path, file_name)
            tasks.append(asyncio.create_task(aio_download_ts(movie_save_path, ts_url, session)))
        # 启动多任务异步下载
        result, pending = await asyncio.wait(tasks)
        # 如果result里有东西. 那就坏菜了.  这里可以考虑让程序休息一会儿. 然后重新下载. 
        # 或者直接记录在文件里. 等以后再下载. 
        

def get_key(second_m3u8):
    obj = re.compile(r'URI="(?P<key_url>.*?)"')
    result = obj.search(second_m3u8)
    key_url = result.group("key_url")
    return get_page_source(key_url)


async def aio_decrypt_ts(file_path, new_file_path, key):
    async with aiofiles.open(file_path, mode="rb") as f1, \
            aiofiles.open(new_file_path, mode="wb") as f2:

        content = await f1.read()
        aes = AES.new(key.encode("utf-8"), IV=b"0000000000000000", mode=AES.MODE_CBC)
        decrypt_content = aes.decrypt(content)
        await f2.write(decrypt_content)
    print(f"解密成功, 文件被存放在{new_file_path}")


async def aio_decrypt(movie_name, ts_url_list, key):
    file_path_dir = f"./{movie_name}"
    new_file_path_dir = f"./{movie_name}/temp"
    if not os.path.exists(new_file_path_dir):
        os.makedirs(new_file_path_dir)
    tasks = []
    for ts_url in ts_url_list:
        ts_name = ts_url.split("/")[-1]
        file_path = os.path.join(file_path_dir, ts_name)
        new_file_path = os.path.join(new_file_path_dir, ts_name)
        tasks.append(asyncio.create_task(aio_decrypt_ts(file_path, new_file_path, key)))
    result = await asyncio.gather(*tasks)
    return result


def merge(movie_name, ts_url_list):
    new_file_path_dir = f"./{movie_name}/temp"
    # 进入到该文件夹内
    cwd = os.getcwd()
    os.chdir(new_file_path_dir)
    # 合并, 每50个合并为1个.
    part = 1
    last = []
    ts_list = []
    for i in range(len(ts_url_list)):
        ts_url = ts_url_list[i]

        ts_name = ts_url.split("/")[-1]
        ts_list.append(ts_name)

        if i != 0 and i % 50 == 0:
            # "cat  {names} > movie.mp4"
            os.popen(f"cat  {' '.join(ts_list)} > big_movie_{part}.ts")
            last.append(f"big_movie_{part}.ts")
            part += 1
            ts_list = []
    # 最后的最后还剩下一些没有合并呢
    os.popen(f"cat  {' '.join(ts_list)} > big_movie_{part}.ts")
    last.append(f"big_movie_{part}.ts")

    os.popen(f"cat  {' '.join(last)} > movie.mp4")
    os.chdir(cwd)


def parse_m3u8(movie_name, first_m3u8_url):
    print("开始解析M3U8中的ts文件......")
    first_m3u8 = get_page_source(first_m3u8_url)
    second_m3u8_url = ""
    for item in first_m3u8.split():
        if not item.startswith("#"):
            second_m3u8_url = parse.urljoin(first_m3u8_url, item)
            break

    second_m3u8 = get_page_source(second_m3u8_url)
    with open("second_m3u8.txt", mode="w") as f:
        f.write(second_m3u8)
    ts_url_list = []
    for item in second_m3u8.split():
        if not item.startswith("#"):
            # 开始下载
            ts_url_list.append(item)

    print("开始下载M3U8中的ts文件..疯狂下载中.....")
    # 启动协程, 下载ts文件
    asyncio.run(aio_download(movie_name, ts_url_list))
    print("ts文件下载完毕..即将进入解密阶段.....")

    print("开始解密..获取解密秘钥key..")
    key = get_key(second_m3u8)
    print(key)
    # 启动协程, 下载ts文件
    asyncio.run(aio_decrypt(movie_name, ts_url_list, key))
    print("ts文件下载完毕..即将进入合并阶段.....")

    # 合并
    merge(movie_name, ts_url_list)


def main():
    url = "http://www.wbdy.tv/play/30288_1_1.html"
    print(f"开始解析{url}中的m3u8内容....")
    movie_name, iframe_url = get_iframe_url(url)
    iframe_url = parse.urljoin(url, iframe_url)
    first_m3u8_url = get_m3u8_file_url(iframe_url)
    print(f"解析到视频中的m3u8内容....")
    parse_m3u8(movie_name, first_m3u8_url)

    print(f"全部搞定. 接下来. 你可以开始看电影了. ")


if __name__ == '__main__':
    main()

```

### X 视频

### X 站

### X 头条

### 得物 APP

### B 站

### 抖音

### 知乎

## Reference
