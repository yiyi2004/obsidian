- Time：2023-05-31 10:01
- Label： #python #spider #ddos

## Abstract

```ad-abstract
title:爬虫高级总结

1. 异步爬虫
2. 逆向爬虫
3. APP 逆向入门
4. Java 开发基础
5. 安卓开发
6. 安卓和 JUI 开发
7. Flask服务
8. Flask服务和平台
```

## Content

### Thread

创建线程的两种方式

```python
from threading import Thread


def func():
    for i in range(1000):
        print("func", i)


if __name__ == '__main__':
    t = Thread(target=func)
    t.start()
    for i in range(1000):
        print("main", i)
```

```python
from threading import Thread


class MyThread(Thread):
    def run(self):
        for i in range(1000):
            print("func", i)


if __name__ == '__main__':
    t = MyThread()
    t.start()
    for i in range(1000):
        print("main", i)
```

创建线程池

```python
# 线程池
def fn(name):
    for i in range(1000):
        print(name, i)


if __name__ == '__main__':
    with ThreadPoolExecutor(10) as t:
        for i in range(100):
            t.submit(fn, name=f"线程{i}")

```

如何获取返回值？

```python
def func(name):
    time.sleep(2)
    return name


def do_callback(res):
    print(res.result())


if __name__ == '__main__':
    with ThreadPoolExecutor(10) as t:
        names = ["线程1", "线程2", "线程3"]
        for name in names:
            # 方案一, 添加回调
            t.submit(func, name).add_done_callback(do_callback)

            
if __name__ == '__main__':
    start = time.time()
    with ThreadPoolExecutor(10) as t:
        names = [5, 2, 3]
        # 方案二, 直接用map进行任务分发. 最后统一返回结果
        results = t.map(func, names, )  # 结果是按照你传递的顺序来执行的, 代价就是如果第一个没结束. 后面就都没结果
        for r in results:
            print("result", r)
    print(time.time() - start)
```

多线程在爬虫中的应用

```python
import requests
from lxml import etree
from concurrent.futures import ThreadPoolExecutor


def get_page_source(url):
    resp = requests.get(url)
    return resp.text


def get_totle_count():
    url = "http://www.xinfadi.com.cn/marketanalysis/0/list/1.shtml"
    source = get_page_source(url)
    tree = etree.HTML(source)
    last_href = tree.xpath("//div[@class='manu']/a[last()]/@href")[0]
    totle = last_href.split("/")[-1].split(".")[0]
    return int(totle)


def download_content(url):
    source = get_page_source(url)
    tree = etree.HTML(source)
    trs = tree.xpath("//table[@class='hq_table']/tr[position() > 1]")
    result = []
    for tr in trs:
        tds = tr.xpath("./td/text()")
        result.append((tds[0], tds[1], tds[2], tds[3], tds[4], tds[5], tds[6]))
    return result


def main():
    f = open("data.csv", mode="w")
    totle = get_totle_count()
    url_tpl = "http://www.xinfadi.com.cn/marketanalysis/0/list/{}.shtml"

    with ThreadPoolExecutor(50) as t:
        data = t.map(download_content, (url_tpl.format(i) for i in range(1, totle+1)))
        # 拿到所有任务的返回
        for item in data:
            # 每个任务的数据循环出一行
            for detial in item:
                # 写入文件
                content = ",".join(detial) + "\n"
                print(content)
                f.write(content)


if __name__ == '__main__':
    main()
```

### Process

多线程一般是相同任务，多进程一般是不同任务。

```python
def func():
    for i in range(1000):
        print("func", i)


if __name__ == '__main__':
    p = Process(target=func)
    p.start()

    for i in range(1000):
        print("main", i)
```

```python
class MyProcess(Process):
    def run(self):
        for i in range(1000):
            print("MyProcess", i)


if __name__ == '__main__':
    t = MyProcess()
    t.start()
    for i in range(1000):
        print("main", i)
```

```ad-note
title:多进程

我们一般很少直接使用多进程. 最适合使用多进程的情况是: 多个任务需要一起执行. 并且互相之间数据可能有交汇但功能相对独立.比如, 我们自己做一个代理 IP 池, 就需要从网络上进行抓取, 抓取得到的 IP 要进行校验才可以进行使用. 此时, 抓取任务和校验任务就相当于完全独立的两个功能. 此时就可以启动多个进程来实现. 再比如, 如果遇到图片抓取的时候, 我们知道图片在一般都在网页的 img 标签中 src 属性存放的是图片的下载地址. 此时我们可以采用多进程的方案来实现, 一个负责疯狂扫图片下载地址. 另一个进程只负责下载图片.

综上, 多个任务需要并行执行, 但是任务之间相对独立 (不一定完全独立). 可以考虑用多进程.
```

```python
# 进程1. 从图片网站中提取到图片的下载路径
def get_pic_src(q):
    print("start main page spider")
    url = "http://www.591mm.com/mntt/"
    resp = requests.get(url)
    tree = etree.HTML(resp.text)
    child_hrefs = tree.xpath("//div[@class='MeinvTuPianBox']/ul/li/a/@href")
    print("get hrefs from main page", child_hrefs)
    for href in child_hrefs:
        href = parse.urljoin(url, href)
        print("handle href", href)
        resp_child = requests.get(href)
        tree = etree.HTML(resp_child.text)
        pic_src = tree.xpath("//div[@id='picBody']//img/@src")[0]
        print(f"put {pic_src} to the queue")
        q.put(pic_src)
        # 作业, 分页图片抓取
        # print("ready to another!")
        # others = tree.xpath('//ul[@class="articleV2Page"]')
        # if others:


# 进程2. 从图片网站中提取到图片的下载路径
def download(url):
    print("start download", url)
    name = url.split("/")[-1]
    resp = requests.get(url)
    with open(name, mode="wb") as f:
        f.write(resp.content)
    resp.close()
    print("downloaded", url)


def start_download(q):
    with ThreadPoolExecutor(20) as t:
        while True:
            t.submit(download, q.get())  # 启动

            
def main():
    q = Queue()
    p1 = Process(target=start_download, args=(q,))
    p2 = Process(target=get_pic_src, args=(q,))
    p1.start()
    p2.start()


if __name__ == '__main__':
    main()
```

### Coroutine

![[Snipaste/Pasted image 20230604201349.png]]

Basic Usage

```python
async def func():
    print("我是协程")


if __name__ == '__main__':
    # print(func())  # 注意, 此时拿到的是一个协程对象, 和生成器差不多.该函数默认是不会这样执行的

    coroutine = func()
    asyncio.run(coroutine)  # 用asyncio的run来执行协程.
    # lop = asyncio.get_event_loop()
    # lop.run_until_complete(coroutine)   # 这两句顶上面一句
    
```

进阶使用方法

```python
import time

# await: 当该任务被挂起后,CPU会自动切换到其他任务中
async def func1():
    print("func1, start")
    await asyncio.sleep(3)
    print("func1, end")


async def func2():
    print("func2, start")
    await asyncio.sleep(4)
    print("func2, end")


async def func3():
    print("func3, start")
    await asyncio.sleep(2)
    print("func3, end")


if __name__ == '__main__':
    start = time.time()
    tasks = [  # 协程任务列表
        asyncio.create_task(func1()),  # 创建协程任务
        asyncio.create_task(func2()),
        asyncio.create_task(func3()),
    ]
    lop = asyncio.get_event_loop()
    # 我要执行这个协程任务列表中的所有任务
    lop.run_until_complete(asyncio.wait(tasks))  # 我要执行这个协程任务列表中的所有任务
    print(time.time() - start) 
```

另外的写法

```python
async def main():
    print("start")
    # # 添加协程任务
    # t1 = asyncio.create_task(func1())
    # t2 = asyncio.create_task(func2())
    # t3 = asyncio.create_task(func3())
    #
    # ret1 = await t1
    # ret2 = await t2
    # ret3 = await t3

    tasks = [
        asyncio.create_task(func1()),
        asyncio.create_task(func2()),
        asyncio.create_task(func3())
    ]
    # 一次性把所有任务都执行
    done, pedding = await asyncio.wait(tasks)
    
    print("end")

if __name__ == '__main__':
    start = time.time()
    asyncio.run(main())
    print(time.time() - start)
```

```python
async def download(url):
    print("开始抓取")
    await asyncio.sleep(3)  # 我要开始下载了
    print("下载结束", url)
    return "老子是源码你信么"


async def main():
    urls = [
        "http://www.baidu.com",
        "http://www.h.com",
        "http://luoyonghao.com"
    ]
    # 生成任务列表
    tasks = [asyncio.create_task(download(url)) for url in urls]
    done, pedding = await asyncio.wait(tasks)
    for d in done:
        print(d.result())


if __name__ == '__main__':
    asyncio.run(main())
```

多任务协程返回值

```python
import asyncio


async def faker1():
    print("任务1开始")
    await asyncio.sleep(1)
    print("任务1完成")
    return "任务1结束"


async def faker2():
    print("任务2开始")
    await asyncio.sleep(2)
    print("任务2完成")
    return "任务2结束"


async def faker3():
    print("任务3开始")
    await asyncio.sleep(3)
    print("任务3完成")
    return "任务3结束"


async def main():
    tasks = [
        asyncio.create_task(faker3()),
        asyncio.create_task(faker1()),
        asyncio.create_task(faker2()),
    ]
    # 方案一, 用wait, 返回的结果在result中
    result, pending = await asyncio.wait(tasks)
    for r in result:
        print(r.result())
        
    # 方案二, 用gather, 返回的结果在result中, 结果会按照任务添加的顺序来返回数据
    # 	return_exceptions如果任务在执行过程中报错了. 返回错误信息. 
    result = await asyncio.gather(*tasks, return_exceptions=True)
    for r in result:
        print(r)


if __name__ == '__main__':
    asyncio.run(main())

```

协程在爬虫的使用

```powershell
pip install aiohttp
pip install aiofiles
```

```python
import aiohttp
import asyncio
import aiofiles


async def download(url):
    try:
        name = url.split("/")[-1]
        # 创建session对象 -> 相当于requsts对象
        async with aiohttp.ClientSession() as session:
            # 发送请求, 这里和requests.get()几乎没区别, 除了代理换成了proxy
            async with session.get(url) as resp:
                # 读取数据. 如果想要读取源代码. 直接resp.text()即可. 比原来多了个()
                content = await resp.content.read()
                # 写入文件, 用默认的open也OK. 用aiofiles能进一步提升效率
                async with aiofiles.open(name, mode="wb") as f:
                    await f.write(content)
                    return "OK"
    except:
        print(123)
        return "NO"


async def main():
    url_list = [
        "http://pic3.hn01.cn/wwl/upload/2021/06-30/omv2i40essl.jpg",
        "http://pic3.hn01.cn/wwl/upload/2021/06-30/kg3ccicvnqd.jpg",
        "http://pic3.hn01.cn/wwl/upload/2021/06-30/jhw5yhbtyaa.jpg",
        "http://pic3.hn01.cn/wwl/upload/2021/06-30/y1enehg1esu.jpg",
        "http://pic3.hn01.cn/wwl/upload/2021/06-28/2pshuolbhrg.jpg",
    ]
    tasks = []

    for url in url_list:
        # 创建任务
        task = asyncio.create_task(download(url))
        tasks.append(task)

    await asyncio.wait(tasks)


if __name__ == '__main__':
    asyncio.run(main())
```

**目标, 明朝那些事儿** [https://www.zanghaihua.org/mingchaonaxieshier/](https://www.zanghaihua.org/mingchaonaxieshier/)

```python
import asyncio  
import aiofiles  
import aiohttp  
import requests  
from lxml import etree  
import os  
import time  
​  
​  
def get_all_detail_url(url):  
    """  
    获取到所有详情页的url  
    :param url: 主页URL  
    :return: {章节名称:[detail_url, detail_url....]}  
    """  
    resp = requests.get(url)  
    tree = etree.HTML(resp.text)  
    booklist = tree.xpath("//div[@class='booklist clearfix']/span")  
​  
    dic = {}  
    chapter = ""  
    for book in booklist:  
        if 'v' in book.xpath("./@class"):  
            chapter = book.xpath("./text()")[0]  
            dic[chapter] = []  
        else:  
            href = book.xpath("./a/@href")[0]  
            dic[chapter].append(href)  
    return dic  
​  
​  
async def download_one(session, file_path, url):  
    async with session.get(url) as resp:  
        text = await resp.text()  
        tree = etree.HTML(text)  
        title = tree.xpath(".//div[@class='chaptertitle clearfix']/h1/text()")[0]  
        content = "\n".join(tree.xpath(".//div[@id='BookText']/text()")).replace("\u3000", "")  
        async with aiofiles.open(f"./{file_path}/{title}.txt", mode="w", encoding='utf-8') as f:  
            await f.write(content)  
​  
​  
async def download(file_path, urls):  
    tasks = []  
    print(id(asyncio.get_event_loop()))  
    async with aiohttp.ClientSession() as session:  
        for url in urls:  
            tasks.append(asyncio.create_task(download_one(session, file_path, url)))  
        await asyncio.wait(tasks)  
        print(file_path, "done")  
​  
​  
def main():  
    # 拿到目录页中所有详情页的url  
    url = "https://www.zanghaihua.org/mingchaonaxieshier/"  
    detail_urls = get_all_detail_url(url)  
​  
    for name, urls in detail_urls.items():  
        if not os.path.exists(name):  
            os.makedirs(f"./{name}/")  
        asyncio.run(download(name, urls))  
​  
​  
if __name__ == '__main__':  
    start = time.time()  
    main()  
    print(time.time()-start)  
```

uvloop, 可以使 asyncio 更快。事实上，它至少比 nodejs、gevent 和其他 Python 异步框架要快两倍 (传说, 我测试没啥区别) 。基于 uvloop 的 asyncio 的速度几乎接近了 Go 程序的速度。

```python
import asyncio
import uvloop

asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())

print(asyncio.new_event_loop())
print(asyncio.get_event_loop())
```

**作业**，将明朝那些事，分文件夹进行存储 + 文件名分类

进一步的进行分析，其实并没有什么困难捏

## Reference
