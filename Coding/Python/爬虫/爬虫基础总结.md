- Time：2023-05-31 09:59
- Label： #爬虫 #逆向 #python #异步 #分布式爬虫 #爬虫基础知识

## Abstract

这部分内容主要包含爬虫基础内容，包括一些基础包的使用以及一些 python 基础内容，例如 python 异步操作等等。

1. 爬虫概念
2. 数据解析
3. requests 进阶
4. 异步爬虫
5. 如何应对反扒机制

都是比较简单的内容

## Content

### 爬虫概念

善意的爬虫与恶意的爬虫

- 善意的爬⾍, 不破坏被爬取的⽹站的资源 (正常访问, ⼀般频率不⾼, 不窃取⽤户隐私)
- 恶意的爬⾍, 影响⽹站的正常运营 (抢票, 秒杀, 疯狂 solo ⽹站资源造成⽹站宕机)

```python
from urllib.request import urlopen
resp = urlopen("http://www.baidu.com") # 打开 百度
# print(resp.read().decode("utf-8")) # 打印 抓取到的
with open("baidu.html",mode="w", encoding="utf-8") as f: # 创建⽂件
f.write(resp.read().decode("utf-")) # 保存在⽂
```

1. ⻚⾯源代码是执⾏ js 脚本以及⽤户操作之前的服务器返回给我们最原始的内容
2. Elements 中看到的内容是 js 脚本以及⽤户操作之后的当时的⻚⾯显示效果.

Console 窗口与 JS 逆向会有关系  
Source 这⾥能看到该⽹⻚打开时加载的所有内容. 包括⻚⾯**源代码. 脚本. 样式, 图⽚**等等全部内容.  
**Network** 我们⼀般习惯称呼它为抓包⼯具. 在这⾥, 我们能看到当前⽹⻚加载的所有⽹路⽹络请求, 以及请求的详细内容. 这⼀点对我们爬⾍来说⾄关重要.

![[Snipaste/Pasted image 20230531151318.png]]

![[Snipaste/Pasted image 20230531151351.png]]  
![[Snipaste/Pasted image 20230531151420.png]]

#### HTTP

请求头中最常⻅的⼀些重要内容 (爬⾍需要):

1. **User-Agent** : 请求载体的身份标识 (⽤啥发送的请求)
2. **Referer**: 防盗链 (这次请求是从哪个⻚⾯来的? 反爬会⽤到)
3. **cookie**: 本地字符串数据信息 (⽤户登录信息, 反爬的 token)  

响应头中⼀些重要的内容:

1. **cookie**: 本地字符串数据信息 (⽤户登录信息, 反爬的 token)
2. 各种神奇的莫名其妙的字符串 (这个需要经验了, ⼀般都是 token 字样, **防⽌各种攻击和反爬**)

#### Requests

```shell
pip install requests
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple requests
```

**模拟发送请求**

```python
# 案例1. 抓取搜狗搜索内容
kw = input("请输入你要搜索的内容:response =
requests.get(f"https ://www.sogou.com/web?query={kwJ") # 发送get请求

# print(response.text)# 直接拿结果(文本)
with open("sogou.html", mode="w", encoding="utf-8"as f:
	f.write(response.text)
```

![[Snipaste/Pasted image 20230531152043.png]]

```python
# 案例2.抓取百度翻译数据
# 准备参数
kw = input("请输⼊你要翻译的英语单词:")

dic = {
	"kw": kw # 这⾥要和抓包⼯具⾥的参数⼀致.
}

# 请注意百度翻译的sug这个url. 它是通过post⽅式进⾏提交的.所以我们也要模拟post请求
resp = requests.post("https://fanyi.baidu.com/sug", data=dic)

# 返回值是json 那就可以直接解析成json
resp_json = resp.json()

# {'errno': 0, 'data': [{'k': 'Apple', 'v': 'n. 苹果公司，原称苹果电脑公司'....
print(resp_json['data'][0]['v']) # 拿到返回字典中的内容
```

```python
# 案例3: 抓取⾖瓣电影
url = 'https://movie.douban.com/j/chart/top_list'
param = {
	'type': '24',
	'interval_id': '100:90',
	'action':'',
	'start': '0',#从库中的第⼏部电影去取
	'limit': '20',#⼀次取出的个数
}
headers = {
	'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36'
}

response = requests.get(url=url,params=param,headers=headers)
list_data = response.json()
fp = open('./douban.json','w',encoding='utf-8')
json.dump(list_data,fp=fp,ensure_ascii=False)
print('over!!!')
```

1. 爬⾍就是写程序去模拟浏览器⽤来抓取互联⽹上的内容
2. python 中⾃带了⼀个 **urllib** 提供给我们进⾏简易爬⾍的编写
3. requests 模块的简单使⽤, 包括 get, post 两种⽅式的请求. 以及 User-Agent 的介绍.

### 数据解析

1. re 解析
2. bs4 解析
3. xpath 解析
4. pyquery 解析

#### 正则表达式

- [[../../../Basic/正则表达式|正则表达式]]

#### Re 模块

#### 豆瓣 TOP250 电影信息

#### HTML + CSS

#### Bs64

#### 爬取图片

#### Xpath 解析

#### 抓取猪八戒数据库

#### Pyquery 简单入门

#### Pyquery 实战演练

### Requests 进阶

1. 模拟浏览器登录 ->处理 cookie
2. 防盗链处理 -> 抓取梨视频数据
3. 代理 -> 防⽌被封 IP
4. 接⼊第三⽅代理

#### 处理 Cookie 问题

1. session 请求获取 Cookie 并且将其添加到下一次请求的请求头里面
2. 直接复制浏览器的 Cookie

```python
import requests
# 建⽴session
session = requests.session()
# 准备⽤户名密码
data = {
	"loginName": "18614075987",
	"password": "xxxxxx"
}
# UA
headers = {
	"user-agent":"Mozilla/5.0 (Macintosh; Intel
	Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like
	Gecko) Chrome/87.0.4280.141 Safari/537.36"
}
# 登录
resp =
session.post("https://passport.17k.com/ck/user/log
in", data=data, headers=headers)
# cookie中的东⻄
print(session.cookies)
# 带着cookie请求书架
resp =session.get("https://user.17k.com/ck/author/shelf")
print(resp.text)
```

#### 防盗链

```python
import requests
url = "https://www.pearvideo.com/video_1713901"
contId = url.split("_")[1]
print(contId)
videoStatus_url =f"https://www.pearvideo.com/videoStatus.jsp?
contId={contId}&mrd=0.8770894467476524"

headers = {
	"User-Agent": "Mozilla/5.0 (Macintosh; Intel
		Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like
		Gecko) Chrome/87.0.4280.141 Safari/537.36",
	"Referer": url # 防盗链,意义:本次请求是由哪个url产⽣的
}
resp = requests.get(videoStatus_url,
headers=headers)
dic = resp.json()
# print(dic)
systemTime = dic['systemTime']
videoUrl = dic["videoInfo"]['videos']['srcUrl']
videoUrl = videoUrl.replace(systemTime, "cont-"+contId) # 拼接真正的视频url地址
# print(videoUrl)
# 下载视频
with open(f"{contId}.mp4", mode="wb") as f:
	f.write(requests.get(videoUrl).content)
```

- Referer 就是防盗链的操作，他会指明是哪个网站跳转过来的——也可以由此来实现优惠劵活动。

#### 代理

```python
import requests
headers = {
	"User-Agent": "Mozilla/5.0 (Macintosh; Intel
		Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like
		Gecko) Chrome/87.0.4280.141 Safari/537.36",
}
proxies = {
	"https": "https://27.148.248.203:80"
}

resp = requests.get("https://www.baidu.com",
headers=headers, proxies=proxies)
print(resp.text)
```

- 第三方代理

```python
import requests
def get_ip():
	while 1: # 反复提取代理IP
		# 有待完善. 如果代理ip都⽤完了. 怎么办????
		url = "https://dev.kdlapi.com/api/getproxy/?
		orderid=962349361442245&num=100&protocol=2&method=1&an_tr=1&quality=1&format=json&sep=1"
		resp = requests.get(url)
		ips = resp.json()
		if ips['code'] == 0:
			for ip in ips['data']['proxy_list']:
			# 拿到每⼀个ip
			yield ip # ⼀个⼀个返回代理ip
			print("所有IP已经⽤完, 即将更新!") # for 循环结束. 继续提取新IP
		else:
			print("获取代理IP出现异常. 重新获取!")


def spider():
	url = "https://www.baidu.com"
	while 1:
	try:
		proxy_ip = next(gen) # 拿到代理ip
		proxy = {
		"http": "http://" + proxy_ip,
		"https": "https://" + proxy_ip,
		}
		resp = requests.get(url, proxies=proxy)
		resp.encoding = "utf-8"
		return resp.text
	 except:
		 print("报错了. ")


if __name__ == '__main__':
	gen = get_ip() # gen就是代理ip的⽣成器
	for i in range(10):
	spider()
```

- spiderman

## Reference
