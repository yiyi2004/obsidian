torchvision 提供一些常用的神经网络，用于分类的模型。毕设中可能会使用的到。

一些常见的参数
1. root 数据集的位置
2. train true 训练集，false 测试集
3. transform
4. target_transform
5. download true 自己下载，false 

## Dataloader
dataset 和 dataloader
![[Pasted image 20220413140147.png]]
num_works 多进程，windows 下面可能存在问题，设置为 0

- [ ] Debug 功能非常重要哦，要善于使用

![[Pasted image 20220413142304.png]]

![[Pasted image 20220413142619.png]]

这我就不写了吧，其实挺简单的，晚上复习一下应该就会了。

imgs 送入神经网络去训练

## 神经网络的搭建
![[Pasted image 20220413143020.png]]
Moduls 是非常重要的。

![[Pasted image 20220413150700.png]]
![[Pasted image 20220413150811.png]]
- [ ] 关于卷积的概念

![[Pasted image 20220413151911.png]]

![[Pasted image 20220413153232.png]]

weight 是一个卷积核
![[Pasted image 20220413153431.png]]

stride 卷积核走的步数
![[Pasted image 20220413153748.png]]
![[Pasted image 20220413153904.png]]
stride 的取值会影响卷积后的输出

我们往往通过去做一件事情去逃避做另外一件事情，这样是非常不好的。

![[Pasted image 20220413154809.png]]



stride
![[Pasted image 20220413154928.png]]

pedding
![[Pasted image 20220413155013.png]]

![[Pasted image 20220413155316.png]]
padding
![[Pasted image 20220413155423.png]]

![[Pasted image 20220413190517.png]]

#Parrmeters
![[Pasted image 20220413190939.png]]
![[Pasted image 20220413191048.png]]
这个是在 link 里面哦，其实是可以看懂的哦。

kernel 中的值会不断的调整，其实是在某个分布中进行取样的。

what is in_channel and out_channel?

#channel 

![[Pasted image 20220413191535.png]]
![[Pasted image 20220413194530.png]]

一个不是很严谨的方法将 6 channels 转换成 3 channels，垒起来变成平铺了。

![[Pasted image 20220413194642.png]]
![[Pasted image 20220413194757.png]]

最后这里邮电没看懂哦，

[Conv2d 公式](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)

卷积神经网络非常适合类处理图像

## 池化层
最大池化、下采样
上采样是插值，下采样是抽样
![[Pasted image 20220413195824.png]]

![[Pasted image 20220413195949.png]]
dilation 空洞卷积 
#floor #cell
![[Pasted image 20220413200104.png]]

最大池化操作
ceil mode == true: 保留
stride = 3


![[Pasted image 20220413200628.png]]
- [ ] 池化的含义是什么呢？为什么需要池化呢？

#maxpool2d_shape
![[Pasted image 20220413201045.png]]
这些公式是可以写道毕业论文中的哦
![[Pasted image 20220413201828.png]]
数字的类型可能出现问题，解决 **dtype=torch.float32**

![[Pasted image 20220413201945.png]]
为什么要进行最大池化
1. 保证数据的最大特征，减少数据量，为了能够训练的更快 1080p ---> 720p 的
两级结构

![[Pasted image 20220413202327.png]]

![[Pasted image 20220413202421.png]]

尽量保留了输入的信息哦，最大池化的作用，因为数据量实在太大了。

#常规套路
卷积 + 池化 + 非线性激活

那么池化会不会对最终训练的模型造成影响呢。
## 非线性激活
给神经网络引入一些非线性的特质
#relu
![[Pasted image 20220413203442.png]]

![[Pasted image 20220413203530.png]]

![[Pasted image 20220413203550.png]]

![[Pasted image 20220413203719.png]]
inplace 是否在原来的地方进行替换哦

一般来说 inplace = False
![[Pasted image 20220413203828.png]]

![[Pasted image 20220413203925.png]]
非线性越多，越是可以训练出复杂多样的模型，其实蛮有道理的哦。


## 线性层及其他结构
### 正则化层
正则化，可以加快神经网络的训练速度；原来这也可以成为一个方向哦。

![[Pasted image 20220413204136.png]]
![[Pasted image 20220413204659.png]]

![[Pasted image 20220413204719.png]]
BatchNorm2d 参数作为默认就好了。

- [x] **不如去看官方文档**

正则化层用的不多哦（但是论文中出现了）

recurrent layer 文字识别中，特定的网络结构，看具体的需要哦。用到的概率不是很大

Transformer 
![[Pasted image 20220413204934.png]]

### 线性层
![[Pasted image 20220413204948.png]]
线性层比较常用哦。
![[Pasted image 20220413212116.png]]
![[Pasted image 20220413212149.png]]
![[Pasted image 20220413212320.png]]
#torchflatten
![[Pasted image 20220413212423.png]]
![[Pasted image 20220413212540.png]]

基本的网络模型就讲解完成了，快速入门的课程哦。

![[Pasted image 20220413212631.png]]

现在我们已经可以自己搭建一些网络模型了，**torchvision** 提供了一些常用的网络模型，可以讲解一下哦。

![[Pasted image 20220413212747.png]]
可以直接调用网络结构接好了

文字方面、语音方面，可以直接使用

![[Pasted image 20220413220016.png]]

网络模型
![[Pasted image 20220413220511.png]]
也许不需要公式哦，中心对准第一个，padding =2
![[Pasted image 20220413220614.png]]
padding = 2 就是最终的结果
![[Pasted image 20220413221255.png]]

如何测试网络的正确性呢
![[Pasted image 20220413221341.png]]

如果是 10240 那么结果就会报错。

你其实讨厌的是那个曾经的通过去做一件事情去逃避做另外一件事情的自己吧。

![[Pasted image 20220413221606.png]]
对网络进行简单的检验
#sequential
![[Pasted image 20220413221743.png]]
代码更加简洁啦

#add_graph
tensorboard 的用法
![[Pasted image 20220413221845.png]]
tensorboard add_graph
![[Pasted image 20220413222004.png]]

这个示意图是非常重要的


---


### Dropout
![[Pasted image 20220413205016.png]]

随机将 tensor 中的元素变为0，为了防止过拟合。

### Distance function
### Loss function
后面会讲解
![[Pasted image 20220413211306.png]]


loss function 反向传播
![[Pasted image 20220413222450.png]]
tensorboard 

loss function 的公式也许会很绕，但是要关注输入和输出的形状 shape

![[Pasted image 20220413223851.png]]

MSELoss 平方差，这是 2 范式吧。
![[Pasted image 20220413224014.png]]

交叉熵：Cross Entropy
![[Pasted image 20220413230305.png]]

![[Pasted image 20220413230530.png]]
- [ ] 交叉熵的概念还需要进一步理解哦
![[Pasted image 20220413230800.png]]
google 还可以这样用
![[Pasted image 20220413230834.png]]
y 指的是图片所属的类别哦。

#关注输入和输出
1. 计算实际输出和目标之间的差距
2. 为我们更新参数提供一些依据（反向传播）

- [ ] 反向传播和梯度下降 —— 这里需要复习一下

![[Pasted image 20220413231737.png]]
优化器利用梯度对网络中的参数进行更新

一定要用 result_loss.backward 进行反向传播，才会更新里面的 gradient.