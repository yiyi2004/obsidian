# Dropout
Dropout 简单来说是是模型节点随机失活，这样使之不会太依赖数据的某些局部特征。

模型的随机失活最终相当于得到了不同的模型，然后类似于投票取各个子模型普遍认同的解，这样将更具有参考价值。有一些集成学习的意味，最终能提高模型的泛化效果。

- 取平均的作用
- 减少神经元之间的共适应关系
- 更深的模型
- 更宽的模型：涉及的特征更多
- [正则化](https://blog.csdn.net/weixin_41960890/article/details/104891561?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522166590221616782390593883%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=166590221616782390593883&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-104891561-null-null.142^v56^control,201^v3^add_ask&utm_term=%E6%AD%A3%E5%88%99%E5%8C%96%20&spm=1018.2226.3001.4187)：模型的损失函数加入正则项可以防止参数过大，防止过分[拟合](https://so.csdn.net/so/search?q=%E6%8B%9F%E5%90%88&spm=1001.2101.3001.7020)从而提高泛化能力。

# 数据角度
- 更多的数据
- 数据增强
- 更好的特征

# 训练角度
- 小的 batch size
- 提前的结束

# 小结
- 提高模型的效果可以从很多角度出发，包括数据角度、模型设计角度与训练角度，其实就是我们深度学习问题的各个阶段，都有改进与提升的余地。

# Reference
- [(138条消息) 【深度学习】常见的提高模型泛化能力的方法_Swocky的博客-CSDN博客_提升模型泛化能力](https://blog.csdn.net/Swocky/article/details/105717067?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522166590134316782388066119%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=166590134316782388066119&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-2-105717067-null-null.142^v56^control,201^v3^add_ask&utm_term=%E5%A6%82%E4%BD%95%E6%8F%90%E9%AB%98%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%B3%9B%E7%94%A8%E6%80%A7&spm=1018.2226.3001.4187)
