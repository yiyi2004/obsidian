![[Pasted image 20220705142732.png]]

# PPT 制作
解释下题目，2021 年见刊 Neural Computing and Applications

1. 研究问题
	1. APT 定义: 详细内容需要另写一份
		1. 这是百度百科和维基百科给出的一些定义。
		2. 一种具有组织性、特定目标以及长时间持续性的新型网络攻击日益猖獗，国际上常称之为**APT（Advanced Persistent Threat高级持续性威胁）攻击**。
		3. 往往是一种综合性的攻击手段，对个人和组织造成的伤害往往也是很大的。
	2. 比较重要的 APT 事件：
		1. 2010 年 google 极光事件
		2. 2016 年 SWIFT系统攻击事件，导致8100万美元被窃取
		3. 近几年我国也成为 APT 攻击的主要受害国，对我国也造成了很大的经济损失。
		4. 所以对 APT 攻击的研究是非常具有意义的、
2. 方法
	1. 深度神经网络，那我们这个网络的目的是为了提高检测 APT 攻击的准确率，也就是尽可能检测出 APT IP，这里面将 IP 分为 APT IP 和 normal IP。
	2. 论文的题目说的是一种新方法，但是其实这种方法在其他领域早就有先例了。就是讲不同的神经网络拼接在一起，看是否能产生化学反应，产生不错的效果，这招是有效的、
	3. 在讲解模型之前先对数据进行处理。CICFlowMeter
	4. 数据的处理，一些细节，几个概念之间的关系
	5. 接下来介绍几个神经网络：
	6. CNN
		1. 你可能会好奇，为什么会用到CNN呢？
		2. 为什么会选择 CNN
		3. frame、flow 多少的事情，APT frame、APT flow、
	7. MLP
	8. **LSTM**
		1. Long ShortTerm 网络——一般就叫做LSTM——是一种RNN特殊的类型，可以学习长期依赖信息。
		2. 长短期记忆（Long short-term memory, LSTM）是一种特殊的RNN，主要是为了解决长序列训练过程中的梯度消失和梯度爆炸问题。简单来说，就是相比普通的RNN，LSTM能够在更长的序列中有更好的表现。
		3. 为什么选择 LSTM
	9. 几个神经网络以及他们的特点，组合之间的差别。尽量慢一点讲。
3. 实验数据

![[Pasted image 20220713085723.png]]




## 要准备的内容
- [x] APT 的定义，用自己的话说一遍。
- [x] APT 事件，产生严重的后果
	- [x] https://netsecurity.51cto.com/article/704892.html
	- [x] https://www.secrss.com/articles/7530
	- [x] 2010 年美国 APT 事件

![[Pasted image 20220713071308.png]]

![[Pasted image 20220713073724.png]]

![[Pasted image 20220713073758.png]]

gate 被打开的程度。

- signal

![[Pasted image 20220713074410.png]]

![[Pasted image 20220713074537.png]]

![[9f9075d38b0d7d0af68df0a482135b5.jpg]]
![[Pasted image 20220713083825.png]]

我人都傻了，这是什么呀，我人都傻了。这些人都这么牛的嘛？

![[Pasted image 20220713114443.png]]

# 电子文档
1. 问题描述
2. 方案描述
3. 仿真方案
4. 性能描述

[Adversarial Deep Learning for Robust Detection of Binary Encoded Malware](https://arxiv.org/pdf/1801.02950.pdf)

论文链接： [Adversarial Deep Learning for Robust Detection of Binary Encoded Malware](https://arxiv.org/pdf/1801.02950.pdf)
## 问题描述
深度神经网络 (DNN) 最初是作为计算机视觉和语音识别人工智能方法中神经网络的扩展。它们还用于计算机安全应用程序，例如恶意软件检测。开发恶意软件检测模型的一大挑战是智能对手通过明智地扰乱可检测的恶意软件以创建所谓的对抗性示例（Adverarial Examples, AE），即逃避检测的恶意软件变体，去逃避模型的检测。用一组数据就能证明。

Accuracy: **91.9%**  
False Positive Rate: **8.2%**  
False Negative Rate (Evasion Rate): **8.1%**

Attack Method | Evasion Rate (%)  
— — — — — — — — — — — —  
dFGSM — 99.7  
rFGSM — 99.7  
BGA — 99.7  
BCA — 41.7

其中 dFGSM、rFGSM、BGA、BCA 是四种生成 AEs 的方法，用一般方法训练的模型在原有数据集上的逃逸率是 8.1%，而将模型用在通过上述方法生成的 AEs 上时，模型的逃逸率会大幅上升，这说明了模型的泛用性较差，那么要怎样解决这样的问题呢？

## 方案描述
恶意软件（Malware）以多种形式存在（例如，PowerShell、PDF 文件和二进制文件）。 在这里，我们重点关注可移植可执行 (PE) 文件形式的恶意软件。

给定一个 PE 文件，我们想知道它时良性的(benign)还是恶意的(malignant)。 在 ML 中，这其实时一个二分类问题，如果 PE文件时良性的，那么他的标签是 0，如果 PE 文件是恶性的，那么它的标签是 1。 

如何表示一个 PE 文件：我们用一个表示其导入函数的二进制特征向量来表示每个 PE。 换句话说，如果相应的函数由 PE 文件导入，我们将向量对应的一个条目设置为1，否则设置为0。 LIEF 工具提供了一种简洁的方法，如下所示，为每个 PE 文件生成一个 22761 维的特征向量(向量中的每一个分量代表是否导入了某个函数)。

![[Pasted image 20220727134704.png]]

用二进制向量表示 PE 后，接下来要训练一个前馈神经网络用于检测 Malware，其中包含 3 个隐藏层，每层 300 个神经元。 ReLU 激活函数应用于所有 3 × 300 的隐藏神经元。 LogSoftMax 函数应用于输出层的两个神经元，它们对应于两个标签：benign 和 malicious。

![[Pasted image 20220727135645.png]]

我们使用 19,000 个良性 PE 和 19,000 个恶意 PE 来构建我们的训练 (60%)、验证 (20%) 和测试 (20%) 集。训练集被分为 16 个 PE 样本的小批量（8 个良性，8 个恶意）。分类器的参数（用 θ 表示）使用负对数似然损失进行调整，使用 Adam 优化算法，epoch 设置为 150，学习率的初始值设置为 0.001。这是本文的 baseline，这个模型得到的结果就是在问题描述模块中所提到的。我们可以看到通过训练得到的模型的鲁棒性是很差的。如果恶意软件作者改变了他们导入函数的方式，使恶意文件看起来像一个良性文件而不影响其恶意功能。或者恶意软件作者可以更改导入函数的名称和符号。模型的检测的准确率就会大幅下降，逃逸率会明显上升。

给定一组对抗性扰动，这些扰动可能会让而已软件逃避模型的检测。简单的方法就是是尝试所有可能的扰动。虽然这是可能的，但通常是不切实际的。对于每个对抗版本，作者都必须解压 PE，导入几个函数，打包并针对模型进行测试。

另一方面，如果恶意软件作者可以访问模型的内部，那么他可以在最大化模型损失（最小化其准确性）的方向上更改二进制向量（以及分别导入的函数）。从微积分来看，梯度指向最陡峭的上升方向。通过反向传播，恶意软件作者可以计算模型损失相对于其 PE 的二进制指示向量的梯度，并发现逃避检测器所需的变化（根据额外的导入函数）。

通过某种方法生成的 AEs 需要满足模型的两个约束。约束一，得到的特征向量必须是二进制的：只能取0（表示没有导入对应的函数）或1（表示导入了对应的函数）。约束二，得到的向量也必须满足 x_adv /\ x = x。沿着梯度方向步进可能打破上述约束。一种补救方法是将扰动特征向量舍入到二进制域（约束一）并将舍入后的特征向量与原始二进制指示符向量 x 进行或运算（约束二）。我们实现了这种过程的 4 种变体，产生了 4 种不同的攻击方法，即 rFGSM、dFGSM、BGA 和 BCA。这四种算法描述如下图所示：

![[Pasted image 20220727144538.png]]

我们在我们的恶意测试集上应用了 4 种攻击方法，并测量了模型上相应生成的对抗版本的逃逸率。这组数据在问题描述中已经提到过了。

那我们怎么做能提高模型的鲁棒性呢？一种方法是使用 saddle-point formulation，可以将生成的 AEs 合并到我们模型的训练过程中。Saddle-point formulationi 是外部最小化问题（这里是模型的损失最小化）和内部最大化问题（这里是 AEs 的生成）的组合。

我们将公式：

![[Pasted image 20220727153429.png]]

修改为：

![[Pasted image 20220727153018.png]]

其中 $\bar{x}$ 是恶意文件特征向量 x 的对抗版本。 S(x) 是一组允许的对抗性扰动（上面提到的中的约束 I 和 II）。

换句话说，我们使用上述描述的攻击方法作为内部最大化器来创建恶意 PE 的对抗版本，然后再最小化损失（上面公式中的外部最小化），如下所示：

![[Pasted image 20220727153809.png]]

可以看到，在内部和外部问题中，我们都使用了损失函数的梯度。对于前者，我们使用相对于输入特征向量的梯度。对于后者，它是关于神经网络参数的梯度。

## 仿真方案
可移植可执行文件(PE)格式是Windows操作系统中可执行文件的文件格式。该格式封装了Windows操作系统管理包装代码所需的信息。PE文件作为恶意软件广泛使用。我们分别从VirusShare和互联网下载站点创建了恶意和良性PE文件的语料库。

为了标记收集的PEs，我们使用VirusTotal的病毒检测器集合。其中有34,995个恶意PE和19，696 个良性PE。对于如何表示一个 PE，在方案描述中有提到。
pybloomfiltermmap
代码实现使用 Pytorch 实现，编程语言为 Python。
Adversarial Deep Learning for Robust Detection of Binary Encoded Malware
## 性能描述
使用上面的鞍点公式(saddle-point formulation)，我们训练了 4 个模型（与自然模型具有相同的架构，也就是没有引入对抗学习概念的模型），分别对应的是方案模块中的 4 次攻击：rFGSM、dFGSM、BGA 和 BCA。

为了测试强化后的模型，我们测量了恶意测试集的逃逸率，以及它的 4 个对抗版本（由 4 次攻击生成），如下表所示。 这些行对应于经过训练的模型（由它们的内部最大化方法表示）。这些列对应于精心设计的 Adversarial Malware(由四种方法生成的)。

![[Pasted image 20220727154007.png]]

我们看到每个强化模型对它训练时所用到的攻击方法训练出来的模型对该种攻击方法生成的 Adversarial Malware 的鲁棒性是最好的(逃逸率最低)。这与鞍点公式是一致的。

## 总结
该方法是减少神经网络恶意软件检测器的对抗性盲点的方法。我们将其作为二进制域中的鞍点优化问题来解决，并通过多种内部最大化方法来训练DNN，这些方法对数据集的 AEs 具有鲁棒性。

代码的实践并不是独立实现的，论文作者提供了官方的实现方法和数据库，因为所用操作系统不同，需要对代码进行一定的修改(作者提供了 OSX 和 Linux 上的实现)。完整的代码参考https://github.com/ALFA-group/robust-adv-malware-detection

在本次实践学习中，我学习到了很多深度对抗神经网络、FGSM 的相关知识，学习并运用 Pytroch 完成了本次实践作业。阅读了一些经典的 GAN、对抗样本生成算法相关文献。通过本次实践，学习到了很多。

## 关键代码
