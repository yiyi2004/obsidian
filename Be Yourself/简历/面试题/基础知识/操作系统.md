1. topN
2. CPU 利用率低
3. 进程之间通信
4. 线程之间通信
5. 共享内存的原理  

## 什么是操作系统？请简要概述一下

1. 软件
2. 管理计算机软硬件资源
	1. 处理器
	2. 存储器
	3. 设备管理
	4. 文件管理
3. 接口

## 什么是内核态和用户态？

1. 避免被破坏
2. 内核态：特权指令
3. 用户态
4. 运行  

为了避免操作系统和关键数据被用户程序破坏，将处理器的执行状态分为内核态和用户态。

内核态是操作系统管理程序执行时所处的状态，能够执行包含特权指令在内的一切指令，能够访问系统内所有的存储空间。

用户态是用户程序执行时处理器所处的状态，不能执行特权指令，只能访问用户地址空间。

用户程序运行在用户态,操作系统内核运行在内核态。

## 如何实现内核态和用户态的切换？

1. 系统调用：接口、软中断
2. 异常：内中断、错误：文件损坏、缺页中断
3. 外部中断

处理器从用户态切换到内核态的方法有三种：系统调用、异常和外部中断。

1. 系统调用是操作系统的最小功能单位，是操作系统提供的用户接口，系统调用本身是一种软中断。
2. 异常，也叫做内中断，是由错误引起的，如文件损坏、缺页故障等。
3. 外部中断，是通过两根信号线来通知处理器外设的状态变化，是硬中断。

## 什么是进程？

1. 抽象概念、单位
2. 执行中程序的实例  
3. 上下文
4. 进程的组成
	1. PCB 进程控制块
		1. PID
		2. 进程当前的状态
		3. 程序和数据地址
		4. 进程优先级
		5. CPU 现场保护区 (用于进程切换)
		6. 咱有资源清单等等
	2. 程序段
	3. 数据段

进程是操作系统中最重要的抽象概念之一，是资源分配的基本单位，是独立运行的基本单位。

进程的经典定义就是一个执行中程序的实例。系统中的每个程序都运行在某个进程的上下文（context）中。

上下文是由程序正确运行所需的状态组成的。这个状态包括存放在内存中的程序的代码和数据，它的栈、通用目的寄存器的内容、程序计数器、环境变量以及打开文件描述符的集合。

进程一般由以下的部分组成：

1. 进程控制块 PCB，是进程存在的唯一标志，包含进程标识符 PID，进程当前状态，程序和数据地址，进程优先级、CPU 现场保护区（用于进程切换），占有的资源清单等。
2. 程序段
3. 数据段

## 进程的基本操作

1. 进程的创建
2. 回收子进程
3. 加载并运行程序
4. 进程终止

以 Unix 系统举例：

1. 进程的创建：fork()。新创建的子进程几乎但不完全与父进程相同。子进程得到与父进程用户级虚拟地址空间相同的 (但是独立的) 一份副本，包括代码和数据段、堆、共享库以及用户栈。子进程还获得与父进程任何打开文件描述符相同的副本，这就意味着当父进程调用 fork 时，子进程可以读写父进程中打开的任何文件。父进程和新创建的子进程之间最大的区别在于它们有不同的 PID。fork 函数是有趣的（也常常令人迷惑）， 因为它只被调用一次，却会返回两次：一次是在调用进程（父进程）中，一次是在新创建的子进程中。在父进程中，fork 返回子进程的 PID。在子进程中，fork 返回 0。因为子进程的 PID 总是为非零，返回值就提供一个明 确的方法来分辨程序是在父进程还是在子进程中执行。

    ```c
    pid_t fork(void);
    ```

2. 回收子进程：当一个进程由于某种原因终止时，内核并不是立即把它从系统中清除。相反，进程被保持在一种已终止的状态中，直到被它的父进程回收（reaped）。当父进程回收已终止的子进程时，内核将子进程的退出状态传递给父进程，然后抛弃已终止的进程。一个进程可以通过调用 waitpid 函数来等待它的子进程终止或者停止。

    ```c
    pid_t waitpid(pid_t pid, int *statusp, int options);
    ```

3. 加载并运行程序：execve 函数在当前进程的上下文中加载并运行一个新程序。

    ```c
    int execve(const char *filename, const char *argv[], const char *envp[]);
    ```

4. 进程终止：

    ```c
    void exit(int status);
    ```

## 简述进程间通信方法

1. 拷贝到内核缓冲区
2. 提供这种资源的形式或者提供者不同，造成了通信方式的不同
3. 通信方式：管道、系统 IPC（消息队列、信号量、信号、共享内存等）、以及套接字 Socket。

每个进程各自有不同的用户地址空间,任何一个进程的全局变量在另一个进程中都看不到，所以进程之间要交换数据必须通过内核,在内核中开辟一块缓冲区,进程 A 把数据从用户空间拷到内核缓冲区,进程 B 再从内核缓冲区把数据读走,内核提供的这种机制称为进程间通信。

不同进程间的通信本质：进程之间可以看到一份公共资源；而提供这份资源的形式或者提供者不同，造成了通信方式不同。

进程间通信主要包括管道、系统 IPC（包括消息队列、信号量、信号、共享内存等）、以及套接字 socket。

## 进程如何通过管道进行通信

1. 血缘关系的进程之间进行通信
2. 特质
	1. 其本质是一个伪文件 (实为内核缓冲区)
	2. 由两个文件描述符引用，一个表示读端，一个表示写端。
	3. 规定数据从管道的写端流入管道，从读端流出。  
3. 原理：内核环形队列机制，借助内核缓冲区实现。
4. 局限性：
	1. 数据一致性 读写锁
	2. 不能反复读取
	3. 半双工、单向
	4. 血缘、公共祖先

管道是一种最基本的 IPC 机制，作用于有血缘关系的进程之间，完成数据传递。调用 pipe 系统函数即可创建一个管道。有如下特质：

1. 其本质是一个伪文件 (实为内核缓冲区)
2. 由两个文件描述符引用，一个表示读端，一个表示写端。
3. 规定数据从管道的写端流入管道，从读端流出。

管道的原理: 管道实为内核使用环形队列机制，借助内核缓冲区实现。

管道的局限性：

1. 数据自己读不能自己写。**数据一致性，读的时候不允许写**
2. 数据一旦被读走，便不在管道中存在，不可反复读取。
3. 由于管道采用半双工通信方式。因此，数据只能在一个方向上流动。
4. 只能在有公共祖先的进程间使用管道。

## 进程如何通过共享内存通信 (如何实现内存共享机制)？

1. 什么是共享内存
2. 依靠同步操作实现，互斥锁和信号量等
3. 特点
	1. 内存操作、避免用户空间和内核空间数据的拷贝
	2. 可以多个进程同时操作，所以需要进行同步处理
	3. 信号量和共享内存通常结合在一起使用，信号量用来同步对共享内存的访问。

它使得多个进程可以访问同一块内存空间，不同进程可以及时看到对方进程中对共享内存中数据得更新。这种方式需要依靠某种同步操作，如互斥锁和信号量等。

特点：

1. 共享内存是最快的一种 IPC，因为进程是直接对内存进行操作来实现通信，避免了数据在用户空间和内核空间来回拷贝。
2. 因为多个进程可以同时操作，所以需要进行同步处理。
3. 信号量和共享内存通常结合在一起使用，信号量用来同步对共享内存的访问。

## 什么是信号

1. 什么是信号：

一个信号就是一条小消息，它通知进程系统中发生了一个某种类型的事件。 Linux 系统上支持的 30 种不同类型的信号。 每种信号类型都对应于某种系统事件。低层的硬件异常是由内核异常处理程序处理的，正常情况下，对用户进程而言是不可见的。信号提供了一种机制，通知用户进程发生了这些异常。

1. 发送信号：内核通过更新目的进程上下文中的某个状态，发送（递送）一个信号给目的进程。发送信号可以有如下两种原因：

    + 内核检测到一个**系统事件**，比如除零错误或者子进程终止。
    + —个进程调用了 kill 函数， 显式地要求内核发送一个信号给目的进程。一个进程可以发送信号给它自己。

2. 接收信号：当目的进程被内核强迫以某种方式对信号的发送做出反应时，它就接收了信号。进程可以忽略这个信号，终止或者通过执行一个称为信号处理程序 (signal handler) 的用户层函数捕获这个信号。

## 进程调度的时机

1. 当前运行的**进程运行结束**。
2. 当前运行的进程由于某种原因**阻塞**。
3. **执行完系统调用**等系统程序后返回用户进程。
4. 在使用**抢占调度的系统**中，具有**更高优先级**的进程就绪时。
5. **分时系统**中，分给当前进程的**时间片用完**。

## 不能进行进程调度的情况

1. 在中断处理程序执行时。
2. 在操作系统的内核程序临界区内。
3. 其它需要完全屏蔽中断的原子操作过程中。

## 进程的调度策略

1. 先到先服务调度算法
2. 短作业优先调度算法
3. 优先级调度算法
4. 时间片轮转调度算法
5. 高响应比优先调度算法 (等待时间 + 服务时间) / 服务时间
6. **多级队列调度算法**
7. **多级反馈队列调度算法**

## 进程调度策略的基本设计指标

1. CPU 利用率
2. 系统吞吐率，即单位时间内 CPU 完成的作业的数量。
3. 响应时间。
4. 周转时间。是指作业从提交到完成的时间间隔。从每个作业的角度看，完成每个作业的时间也是很关键
	1. 平均周转时间
	2. 带权周转时间
	3. 平均带权周转时间

## 进程的状态与状态转换

进程在运行时有三种基本状态：就绪态、运行态和阻塞态。

1. 运行（running）态：进程占有处理器正在运行的状态。进程已获得 CPU，其程序正在执行。在单处理机系统中，只有一个进程处于执行状态； 在多处理机系统中，则有多个进程处于执行状态。
2. 就绪（ready）态：进程具备运行条件，等待系统分配处理器以便运行的状态。 当进程已分配到除 CPU 以外的所有必要资源后，只要再获得 CPU，便可立即执行，进程这时的状态称为就绪状态。在一个系统中处于就绪状态的进程可能有多个，通常将它们排成一个队列，称为就绪队列。
3. 阻塞（wait）态：又称等待态或睡眠态，指进程不具备运行条件，正在等待某个时间完成的状态。

各状态之间的转换：

1. 就绪→执行 处于就绪状态的进程，当进程调度程序为之分配了处理机后，该进程便由就绪状态转变成执行状态。
2. 执行→就绪 处于执行状态的进程在其执行过程中，因分配给它的一个时间片已用完而不得不让出处理机，于是进程从执行状态转变成就绪状态。
3. 执行→阻塞 正在执行的进程**因等待某种事件发生而无法继续执行**时，便从执行状态变成阻塞状态。
4. 阻塞→就绪 处于阻塞状态的进程，若其等待的事件已经发生，于是进程由阻塞状态转变为就绪状态。

## 什么是孤儿进程？僵尸进程?

1. 孤儿进程： 父进程退出，子进程还在运行的这些子进程都是孤儿进程，孤儿进程将被 **init 进程（1 号进程）所收养**，并由 init 进程对他们完成状态收集工作。
2. 僵尸进程： 进程使用 fork 创建子进程，如果子进程退出，而父进程并没有调用 **wait 获 waitpid 获取子进程的状态信息**，那么子进程的进程描述符仍然保存在系统中的这些进程是僵尸进程。

## 什么是线程？

1. 是进程划分的任务，是一个进程内可调度的实体，是 CPU 调度的基本单位，用于保证程序的实时性，实现进程内部的并发。
2. 线程是操作系统可识别的最小执行和调度单位。每个线程都独自占用一个虚拟处理器：独自的寄存器组，指令计数器和处理器状态。
3. 每个线程完成不同的任务，但是属于同一个进程的不同线程之间共享同一地址空间（也就是同样的动态内存，映射文件，目标代码等等），打开的文件队列和其他内核资源。

## 为什么需要线程？

线程产生的原因：进程可以使多个程序能并发执行，以提高资源的利用率和系统的吞吐量；但是其具有一些缺点：

1. 进程在同一时刻只能做一个任务，很多时候不能充分利用 CPU 资源。
2. 进程在执行的过程中如果发生阻塞，整个进程就会挂起，即使进程中其它任务不依赖于等待的资源，进程仍会被阻塞。

引入线程就是为了解决以上进程的不足，线程具有以下的优点：

1. 从资源上来讲，开辟一个线程所需要的资源要远小于一个进程。
2. 从切换效率上来讲，运行于一个进程中的多个线程，它们之间使用相同的地址空间，而且线程间彼此切换所需时间也远远小于进程间切换所需要的时间（这种时间的差异主要由于缓存的大量未命中导致）。
3. 从通信机制上来讲，线程间方便的通信机制。对不同进程来说，它们具有独立的地址空间，要进行数据的传递只能通过进程间通信的方式进行。线程则不然，属于同一个进程的不同线程之间共享同一地址空间，所以一个线程的数据可以被其它线程感知，线程间可以直接读写进程数据段（如全局变量）来进行通信（需要一些同步措施）。

## 简述线程和进程的区别和联系

1. 一个线程只能属于一个进程，而一个进程可以有多个线程，但至少有一个线程。线程依赖于进程而存在。
2. 进程在执行过程中拥有独立的地址空间，而多个线程共享进程的地址空间。（资源分配给进程，同一进程的所有线程共享该进程的所有资源。同一进程中的多个线程共享代码段（代码和常量），数据段（全局变量和静态变量），扩展段（堆存储）。但是每个线程拥有自己的栈段，栈段又叫运行时段，用来存放所有局部变量和临时变量。）
3. 进程是资源分配的最小单位，线程是 CPU 调度的最小单位。
4. 通信：由于同一进程中的多个线程具有相同的地址空间，使它们之间的同步和通信的实现，也变得比较容易。进程间通信 `IPC`，线程间可以直接读写进程数据段（如全局变量）来进行通信（需要一些同步方法，以保证数据的一致性）。
5. 进程编程调试简单可靠性高，但是创建销毁开销大；线程正相反，开销小，切换速度快，但是编程调试相对复杂。
6. 进程间不会相互影响；一个进程内某个线程挂掉将导致整个进程挂掉。
7. 进程适应于多核、多机分布；线程适用于多核。

## 进程和线程的基本 API

进程 API 以 Unix 系统为例，线程相关的 API 属于 Posix 线程 (Pthreads) 标准接口。

| 进程原语  |       线程原语        |             描述             |
| :-------: | :-------------------: | :--------------------------: |
|  `fork`   |   `pthread_create`    |        创建新的控制流        |
|  `exit`   |    `pthread_exit`     |     从现有的控制流中退出     |
| `waitpid` |    `pthread_join`     |    从控制流中得到退出状态    |
| `atexit`  | `pthread_cancel_push` | 注册在退出控制流时调用的函数 |
| `getpid`  |    `pthread_self`     |        获取控制流的 ID        |
|  `abort`  |   `pthread_cancel`    |    请求控制流的非正常退出    |

## 多线程模型

1. 多对一模型。将多个用户级线程映射到一个内核级线程上。该模型下，线程在用户空间进行管理，效率较高。缺点就是一个线程阻塞，整个进程内的所有线程都会阻塞。几乎没有系统继续使用这个模型。
2. 一对一模型。将内核线程与用户线程一一对应。优点是一个线程阻塞时，不会影响到其它线程的执行。该模型具有更好的并发性。缺点是内核线程数量一般有上限，会限制用户线程的数量。更多的内核线程数目也给线程切换带来额外的负担。linux 和 Windows 操作系统家族都是使用一对一模型。
3. 多对多模型。将多个用户级线程映射到多个内核级线程上。结合了多对一模型和一对一模型的特点。

## 线程间通信的方式

1. 临界区：通过多线程的串行化来访问公共资源，速度快，适合控制数据访问。
2. 锁机制 `Synchronized/Lock`：采用互斥对象机制，只有拥有互斥对象的线程才有访问公共资源的权限。因为互斥对象只有一个，所以可以保证公共资源不会被多个线程同时访问。
3. 信号量 `Semphare`：为控制具有有限数量的用户资源而设计的，它允许多个线程在同一时刻去访问同一个资源，但一般需要限制同一时刻访问此资源的最大线程数目。
4. 信号，Wait/Notify：通过通知操作的方式来保持多线程同步，还可以方便的实现多线程优先级的比较操作 -->

## 进程同步的方法

操作系统中，进程是具有不同的地址空间的，两个进程是不能感知到对方的存在的。有时候，需要多个进程来协同完成一些任务。  
当多个进程需要对同一个内核资源进行操作时，这些进程便是竞争的关系，操作系统必须协调各个进程对资源的占用，进程的互斥是解决进程间竞争关系的方法。 进程互斥指若干个进程要使用同一共享资源时，任何时刻最多允许一个进程去使用，其他要使用该资源的进程必须等待，直到占有资源的进程释放该资源。  
当多个进程协同完成一些任务时，不同进程的执行进度不一致，这便产生了进程的同步问题。需要操作系统干预，在特定的同步点对所有进程进行同步，这种协作进程之间相互等待对方消息或信号的协调关系称为进程同步。进程互斥本质上也是一种进程同步。  
进程的同步方法：

1. 互斥锁
2. 读写锁
3. 条件变量
4. 记录锁 (record locking)
5. 信号量
6. 屏障（barrier）

## 线程同步的方法

操作系统中，属于同一进程的线程之间具有相同的地址空间，线程之间共享数据变得简单高效。遇到竞争的线程同时修改同一数据或是协作的线程设置同步点的问题时，需要使用一些线程同步的方法来解决这些问题。

线程同步的方法：

1. 互斥锁
2. 读写锁
3. 条件变量
4. 信号量
5. 自旋锁
6. 屏障（barrier）

## Tag

## 进程同步与线程同步有什么区别

进程之间地址空间不同，不能感知对方的存在，同步时需要将锁放在多进程共享的空间。而线程之间共享同一地址空间，同步时把锁放在所属的同一进程空间即可。

## 死锁是怎样产生的？

死锁是指两个或两个以上进程在执行过程中，因争夺资源而造成的下相互等待的现象。  
产生死锁需要满足下面四个条件：

1. 互斥条件：进程对所分配到的资源不允许其他进程访问，若其他进程访问该资源，只能等待，直至占有该资源的进程使用完成后释放该资源。
2. 占有并等待条件：进程获得一定的资源后，又对其他资源发出请求，但是该资源可能被其他进程占有，此时请求阻塞，但该进程不会释放自己已经占有的资源。
3. 非抢占条件：进程已获得的资源，在未完成使用之前，不可被剥夺，只能在使用后自己释放。
4. 循环等待条件：进程发生死锁后，必然存在一个进程 - 资源之间的环形链。

## 如何解决死锁问题？

解决死锁的方法即破坏产生死锁的四个必要条件之一，主要方法如下:

1. **资源一次性分配**，这样就不会再有请求了（破坏请求条件）。
2. **只要有一个资源得不到分配，也不给这个进程分配其他的资源**（破坏占有并等待条件）。
3. **可抢占资源**：即当进程新的资源未得到满足时，释放已占有的资源，从而破坏不可抢占的条件。
4. **资源有序分配法**：系统给每类资源赋予一个序号，每个进程按编号递增的请求资源，释放则相反，从而破坏环路等待的条件

## 什么是虚拟地址，什么是物理地址？

地址空间是一个非负整数地址的有序集合。

在一个带虚拟内存的系统中，CPU 从一个有 N=pow(2,n) 个地址的地址空间中生成虚拟地址，这个地址空间称为虚拟地址空间（virtual address space）,现代系统通常支持 32 位或者 64 位虚拟地址空间。

一个系统还有一个物理地址空间（physical address space），对应于系统中物理内存的 M 个字节。

地址空间的概念是很重要的，因为它清楚地区分了数据对象（字节）和它们的属性（地址）。

 一旦认识到了这种区别，那么我们就可以将其推广，允许每个数据对象有多个独立的地址，其中每个地址都选自一个不同的地址空间。这就是虚拟内存的基本思想。

 主存中的每字节都有一个选自虚拟地址空间的虚拟地址和一个选自物理地址空间的物理地址。

## 什么是虚拟内存？

为了更加有效地管理内存并且少出错，现代系统提供了一种对主存的抽象概念，叫做虚拟内存 (VM)。虚拟内存是硬件异常、硬件地址翻译、主存、磁盘文件和内核软件的完美交互，它为每个进程提供了一个大的、一致的和私有的地址空间。通过一个很清晰的机制，虚拟内存提供了三个重要的能力：

1. 它将主存看成是一个存储在磁盘上的地址空间的高速缓存，在主存中只保存活动区域，并根据需要在磁盘和主存之间来回传送数据，通过这种方式，它高效地使用了主存。
2. 它为每个进程提供了一致的地址空间，从而简化了内存管理。
3. 它保护了每个进程的地址空间不被其他进程破坏。

## 为什么要引入虚拟内存？

1. 虚拟内存作为缓存的工具

    + 虚拟内存被组织为一个由存放在磁盘上的 N 个连续的字节大小的单元组成的数组。
    + 虚拟内存利用 DRAM 缓存来自通常更大的虚拟地址空间的页面。

2. 虚拟内存作为内存管理的工具。操作系统为每个进程提供了一个独立的页表，也就是独立的虚拟地址空间。多个虚拟页面可以映射到同一个物理页面上。

    + **简化链接：** 独立的地址空间允许每个进程的内存映像使用相同的基本格式，而不管代码和数据实际存放在物理内存的何处。
      + 例如：一个给定的 `linux` 系统上的每个进程都是用类似的内存格式，对于 64 为地址空间，代码段总是从虚拟地址）`0x400000` 开始，数据段，代码段，栈，堆等等。
    + **简化加载：** 虚拟内存还使得容易向内存中加载可执行文件和共享对象文件。要把目标文件中.text 和.data 节加载到一个新创建的进程中，Linux 加载器为代码和数据段分配虚拟页 VP，把他们**标记为无效（未被缓存）** ，将页表条目指向目标文件的起始位置。
      + **加载器从不在磁盘到内存实际复制任何数据，在每个页初次被引用时，虚拟内存系统会按照需要自动的调入数据页。**
    + **简化共享：** 独立地址空间为 OS 提供了一个管理用户进程和操作系统自身之间共享的一致机制。
        + 一般：每个进程有各自私有的代码，数据，堆栈，是不和其他进程共享的，**这样 OS 创建页表，将虚拟页映射到不连续的物理页面。**
        + 某些情况下，需要进程来共享代码和数据。例如每个进程调用相同的操作系统内核代码，或者 C 标准库函数。**OS 会把不同进程中适当的虚拟页面映射到相同的物理页面。**
    + **简化内存分配：** 虚拟内存向用户提供一个简单的分配额外内存的机制。当一个运行在用户进程中的程序要求额外的堆空间时（如 `malloc`），OS 分配一个适当 k 大小个连续的虚拟内存页面，并且将他们映射到物理内存中任意位置的 k 个任意物理页面，**因此操作系统没有必要分配 k 个连续的物理内存页面，页面可以随机的分散在物理内存中**。
+ 虚拟内存作为内存保护的工具。不应该允许一个用户进程修改它的只读段，也不允许它修改任何内核代码和数据结构，不允许读写其他进程的私有内存，不允许修改任何与其他进程共享的虚拟页面。每次 CPU 生成一个地址时，`MMU` 会读一个 `PTE`，通过在 `PTE` 上添加一些额外的许可位来控制对一个虚拟页面内容的访问十分简单。

## 常见的页面置换算法

当访问一个内存中不存在的页，并且内存已满，则需要从内存中调出一个页或将数据送至磁盘对换区，替换一个页，这种现象叫做缺页置换。当前操作系统最常采用的缺页置换算法如下：

+ 先进先出 (FIFO) 算法：
  + 思路：置换最先调入内存的页面，即置换在内存中驻留时间最久的页面。
  + 实现：按照进入内存的先后次序排列成队列，从队尾进入，从队首删除。
  + 特点：实现简单；性能较差，调出的页面可能是经常访问的
+ 最近最少使用（`LRU`）算法:
  + 思路： 置换最近一段时间以来最长时间未访问过的页面。根据程序局部性原理，刚被访问的页面，可能马上又要被访问；而较长时间内没有被访问的页面，可能最近不会被访问。
  + 实现：缺页时，计算内存中每个逻辑页面的上一次访问时间，选择上一次使用到当前时间最长的页面
  + 特点：可能达到最优的效果，维护这样的访问链表开销比较大

当前最常采用的就是 `LRU` 算法。

+ 最不常用算法（`Least Frequently Used, LFU`）
  + 思路：缺页时，置换访问次数最少的页面
  + 实现：每个页面设置一个访问计数，访问页面时，访问计数加 1，缺页时，置换计数最小的页面
  + 特点：算法开销大，开始时频繁使用，但以后不使用的页面很难置换

## 请说一下什么是写时复制？

+ 如果有多个进程要读取它们自己的那部门资源的副本，那么复制是不必要的。每个进程只要保存一个指向这个资源的指针就可以了。只要没有进程要去修改自己的“副本”，就存在着这样的幻觉：每个进程好像独占那个资源。从而就避免了复制带来的负担。如果一个进程要修改自己的那份资源“副本”，那么就会复制那份资源，并把复制的那份提供给进程。不过其中的复制对进程来说是透明的。这个进程就可以修改复制后的资源了，同时其他的进程仍然共享那份没有修改过的资源。所以这就是名称的由来：在写入时进行复制。
+ 写时复制的主要好处在于：如果进程从来就不需要修改资源，则不需要进行复制。惰性算法的好处就在于它们尽量推迟代价高昂的操作，直到必要的时刻才会去执行。
+ 在使用虚拟内存的情况下，写时复制（Copy-On-Write）是以页为基础进行的。所以，只要进程不修改它全部的地址空间，那么就不必复制整个地址空间。在 fork() 调用结束后，父进程和子进程都相信它们有一个自己的地址空间，但实际上它们共享父进程的原始页，接下来这些页又可以被其他的父进程或子进程共享。

## 实时操作系统的概念

实时操作系统（Real-time operating system, RTOS），又称即时操作系统，它会按照排序运行、管理系统资源，并为开发应用程序提供一致的基础。 实时操作系统与一般的操作系统相比，最大的特色就是“实时性”，如果有一个任务需要执行，实时操作系统会马上（在较短时间内）执行该任务，不会有较长的延时。这种特性保证了各个任务的及时执行。

## 优先级反转是什么？如何解决

由于多进程共享资源，具有最高优先权的进程被低优先级进程阻塞，反而使具有中优先级的进程先于高优先级的进程执行，导致系统的崩溃。这就是所谓的优先级反转 (Priority Inversion)。其实,优先级反转是在高优级 (假设为 A) 的任务要访问一个被低优先级任务 (假设为 C) 占有的资源时,被阻塞.而此时又有优先级高于占有资源的任务 (C) 而低于被阻塞的任务 (A) 的优先级的任务 (假设为 B) 时,于是,占有资源的任务就被挂起 (占有的资源仍为它占有),因为占有资源的任务优先级很低,所以,它可能一直被另外的任务挂起.而它占有的资源也就一直不能释放,这样,引起任务 A 一直没办法执行.而比它优先低的任务却可以执行。

目前解决优先级反转有许多种方法。其中普遍使用的有 2 种方法：一种被称作优先级继承 (priority inheritance)；另一种被称作优先级极限 (priority ceilings)。

1. 优先级继承 (priority inheritance) 优先级继承是指将低优先级任务的优先级提升到等待它所占有的资源的最高优先级任务的优先级.当高优先级任务由于等待资源而被阻塞时,此时资源的拥有者的优先级将会自动被提升。
2. 优先级天花板 (priority ceilings) 优先级天花板是指将申请某资源的任务的优先级提升到可能访问该资源的所有任务中最高优先级任务的优先级.(这个优先级称为该资源的优先级天花板)。

## CPU 利用率低的原因有哪些

CPU 利用率低可能由多种原因引起。以下是一些可能的原因：

1. **空闲状态：** 如果系统当前没有足够的任务或者任务的负载较轻，CPU 可能会保持较低的利用率。
2. **I/O 瓶颈：** 当应用程序涉及大量的输入/输出操作时（例如文件读写、网络通信），CPU 可能会等待 I/O 操作完成，导致 CPU 利用率下降。
3. **内存瓶颈：** 如果系统内存不足，导致频繁的页面交换（paging）或使用虚拟内存，可能会导致 CPU 等待内存操作完成，降低 CPU 利用率。
4. **进程阻塞：** 一些进程可能由于等待某些资源、锁或条件而被阻塞，这会影响整体系统的 CPU 利用率。
5. **后台任务：** 有时，后台任务或服务可能会消耗一部分 CPU 资源，而用户任务较少，导致整体 CPU 利用率降低。
6. **性能限制：** 硬件性能可能成为 CPU 利用率的限制因素，例如过时的 CPU 架构或处理器速度较慢。
7. **配置错误：** 错误的系统配置或应用程序配置可能导致性能下降，从而影响 CPU 利用率。
8. **故障或错误：** 系统或应用程序中的错误、故障或死锁可能导致 CPU 利用率下降。

为了确定 CPU 利用率低的具体原因，可以通过以下方法进行诊断：

+ 使用系统监控工具（如 top、Task Manager、Performance Monitor）观察 CPU 使用情况，查看具体的 CPU 利用率和负载情况。
+ 分析日志文件以检查是否存在系统错误或性能问题。
+ 使用性能分析工具来追踪应用程序中的性能瓶颈。
+ 检查系统配置和硬件性能。

通过综合以上信息，可以更好地理解 CPU 利用率低的原因，并采取相应的措施来优化系统性能。

## TOPN 问题，如何找到前 100

处理上亿条数据时，通常需要采用一些分布式、并行化的方法，以保证效率和性能。以下是一些建议：

1. **分布式计算框架：** 使用分布式计算框架如 Hadoop、Spark 等，这样可以将任务分发到多个节点上并行处理。
2. **MapReduce：** 利用 MapReduce 编程模型，将任务分为 Map 和 Reduce 两个阶段，通过 Map 阶段进行数据的初步处理和分组，然后在 Reduce 阶段完成最终的处理和计算。
3. **数据分片：** 将数据分片存储在不同的节点上，确保每个节点只处理其分片的数据。这有助于降低每个节点的负担，提高处理效率。
4. **分阶段处理：** 将处理任务分为多个阶段，每个阶段都筛选出一部分数据。通过多阶段处理逐步缩小数据范围，减少每一阶段需要处理的数据量。
5. **采样：** 在处理之前对数据进行采样，从整体上了解数据的分布情况，有助于确定合适的处理策略。
6. **堆排序或优先队列：** 利用堆排序或优先队列，在处理过程中动态地维护前 N 个最大或最小的元素。
7. **Bloom Filter：** 使用 Bloom Filter 来快速排除掉不可能成为前 100 的数据，减少处理的数据量。
8. **索引：** 如果数据能够按照某个属性有序排列，可以考虑使用索引进行快速查找。
9. **数据库技术：** 利用分布式数据库、数据仓库等技术，进行高效的数据查询。

请注意，具体选择哪种方法取决于数据的特性、存储方式以及可用的计算资源。在实际应用中，可能需要综合使用多种技术手段以达到最优的效果。

## 进程和线程的联系和区别？

线程和进程的联系：

**线程是进程当中的⼀条执⾏流程。**

同⼀个进程内多个线程之间可以共享代码段、数据段、打开的⽂件等资源，但每个线程各⾃都有⼀套独⽴的寄存器和栈，这样可以确保线程的控制流是相对独⽴的。

线程与进程的⽐较如下：

+ 调度：**进程是资源（包括内存、打开的⽂件等）分配的单位**，**线程是 CPU 调度的单位**；
+ 资源：进程拥有⼀个完整的资源平台，⽽线程只独享必不可少的资源，如寄存器和栈；
+ 状态：程同样具有就绪、阻塞、执⾏三种基本状态，同样具有状态之间的转换关系；
+ 系统开销：线程能减少并发执⾏的时间和空间开销——创建或撤销进程时，系统都要为之分配或回收系统资源，如内存空间，I/O 设备等，OS 所付出的开销显著大于在创建或撤销线程时的开销，进程切换的开销也远大于线程切换的开销。

## Page Cache

Page Cache（页缓存）是操作系统中用于提高数据访问速度的一种机制，它缓存了硬盘上的数据到系统内存（RAM）中。这种缓存对于提高文件系统的读写性能至关重要，因为从内存中读取数据比从硬盘（或其他形式的持久存储）读取数据要快得多。

### 工作原理

当程序请求读取磁盘上的数据时，操作系统会先检查这些数据是否已经在 Page Cache 中。如果是，系统就可以直接从内存中读取这些数据，而不需要进行较慢的磁盘 I/O 操作，这称为 " 缓存命中 "。如果请求的数据不在缓存中，操作系统会从磁盘读取数据，并将其放入 Page Cache 中，以备后续使用，这称为 " 缓存未命中 "。

### 特点

+ **透明性**：对于使用它的应用程序来说，Page Cache 的工作过程是透明的，无需应用程序进行特别的编程就能自动受益于它。
+ **动态管理**：操作系统会动态管理 Page Cache 的大小和内容，根据系统的内存使用情况和缓存的访问模式自动调整。
+ **写回策略**：操作系统采用 " 延迟写 "（Write-back）或 " 写通 "（Write-through）策略来处理对缓存数据的写操作。在 " 延迟写 " 策略中，写操作首先被缓存，然后在稍后的某个时间点异步写入磁盘。这可以减少磁盘 I/O 次数，提高性能。
+ **脏页**：在 Page Cache 中被修改但尚未写回到磁盘的页面称为 " 脏页 "。操作系统会定期或在特定条件下将这些脏页写回磁盘。

### 对性能的影响

Page Cache 显著提高了文件访问的速度，特别是对于频繁访问的文件。通过减少磁盘 I/O 操作的需求，它可以提高系统的整体性能和响应速度。然而，Page Cache 需要占用一部分系统内存，因此在内存受

## Mysql 死锁案例

假设有两个用户 A 和 B，同时要更新数据库中的两条记录，但是它们的更新顺序不同，可能会导致死锁。具体例子如下：

```sql
-- 创建一个测试表
CREATE TABLE `accounts` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `balance` int(11) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB;

-- 插入两条记录
INSERT INTO `accounts` (`balance`) VALUES (100), (200);
```

现在，用户 A 想要执行如下的更新操作：

```sql
-- 用户 A 的更新操作
BEGIN;
UPDATE `accounts` SET `balance` = `balance` - 50 WHERE `id` = 1;
-- 此时用户 A 持有 id = 1 的行的写锁

-- 用户 B 的更新操作
BEGIN;
UPDATE `accounts` SET `balance` = `balance` + 50 WHERE `id` = 2;
-- 用户 B 想要更新 id = 2 的行，但是此行已经被用户 A 加锁，用户 B 需要等待

-- 用户 A 继续更新操作
UPDATE `accounts` SET `balance` = `balance` + 50 WHERE `id` = 2;
-- 用户 A 想要更新 id = 2 的行，但是此行已经被用户 B 加锁，用户 A 需要等待
```

此时就发生了死锁。用户 A 持有 id = 1 的行的写锁并等待 id = 2 的行的锁，而用户 B 则持有 id = 2 的行的写锁并等待 id = 1 的行的锁，导致了循环等待，从而产生了死锁。MySQL 会检测到死锁的发生，并通过回滚其中一个事务来解除死锁。
