# 梯度下降法
1. 现实中很难使用
	1. 计算量大

优化两个思路
1. 调整神经网络结构
	1. 池化层
	2. dropout 方法
2. 算法本身上进行优化

下面的内容是对梯度下降法的优化哦

随机梯度下降、动量法、AdaGrad 方法

损失函数的梯度

迭代很多次，一板一眼不实用

2012 年的 alexnet 提出来，不做优化，梯度下降法只能停留在理论层面。我们有哪些思路呢？
1. 能不能减少每一次计算的计算量
2. 优化路径，用更少的步数，更快的到达极值点

# 随机梯度下降法
第一种优化方法。

每次只拿一小部分的数据计算期望，期望从某种程度上来说可以代表整个样本。

![[Pasted image 20220508105239.png]]
凸问题

![[Pasted image 20220508105320.png]]

我说你们能不能先把这个恋爱喜剧的理念先搞懂

梯度下降法的性价比太低。

现在的 SGD：随机挑一个批次，mini batch


随机梯度下降法挑选的样本的梯度是随机的，所以有可能和最终的优化方向不一致。

无限小的计算是非常消耗计算资源的，所以有计算步长的概念（Learning rate）。
![[Pasted image 20220508105820.png]]
可能会有以上这种情况，路径非最优。

学习步数越多，计算量越大。


# 牛顿法
所以要找到一种方法，既保证一定的步长，又能贴合最优优化路径。(数学上找方法)

牛顿法：用二次曲线去代替一次曲线去逼近（举例）。

这里直接拿泰勒展开理解，最优路线就是贴着山走，将贴着山的曲线泰勒展开，牛顿法相当于两项来逼近曲线，梯度下降相当于一项，更高阶理论上更好，但是复杂度更大了

这其实就是泰勒展开式哦。

![[Pasted image 20220508110639.png]]

## Hessian
![[Pasted image 20220508110912.png]]


# 动量法
但是牛顿法的计算量太大了哦。牛顿法是将所有维度防止一起去考虑

![[Pasted image 20220508112513.png]]

将优化的路径划分为两个方向，可以看到其实是纵轴的影响比较大（不严谨）

利用历史的数据去修正这一分量，应该是增加不同的权值。

梯度下降法公式。
![[Pasted image 20220508113256.png]]


历史上的维度相反 ---> 减小，历史上维度同向 ---> 增加（步长更长）
![[Pasted image 20220508113213.png]]
![[Pasted image 20220508113213.png]]
存在问题：
1. 如果步数够多，前面的历史数据可能不再具备参考意义了。可能根本没有参考价值。

![[Pasted image 20220508113542.png]]

距离当前越近的点，可能越是和当前梯度相关的。

**指数加权移动平均法**

![[Pasted image 20220508113719.png]]

# Nesterov(有问题，需要查一查资料)
我们不仅可以参考过去的数据，我们还可以参考将来的数据。

![[Pasted image 20220508150900.png]]

怎么做到超前呢？

是不是没有超过绿色线啊,感觉是应该比之前的橘色线更靠近绿色线,这样的话,感觉就能跟牛顿法扯上关系了

这里其实讲的是有问题的哦。

![[Pasted image 20220508151619.png]]

![[Pasted image 20220508151648.png]]

上面的方法其实是对 0 次项进行的修正。哪可不可以对 1 次项进行修正呢？


学习率不应该是一个固定的值。可能导致不收敛，不论什么方法，学习率都总是不断变小的。一个简单的想法：
1. 每次减小固定的值
2. 学习率自动调节



# AdaGrad
![[Pasted image 20220508152029.png]]

我觉得不应该解释成先平方再开方，应该解释成梯度的内积开方，学习到的梯度是真实梯度除以梯度内积的开方。adagrad本质是解决各方向导数数值量级的不一致而将梯度数值归一化。

上面这句话我怎么听不懂呢？

![[Pasted image 20220508152439.png]]
![[Pasted image 20220508152553.png]]

特点
1. AdaGrad 特别适合稀疏数据：
	1. 维度的概念，其实就是特征啊。
	2. 稀疏矩阵：不同体现在特征的不同，而不是特征程度的不同。
	3. 随着维度的增加，你遇到稀疏数据的可能性就越高。

## 存在问题
![[Pasted image 20220508153525.png]]
在平台期优化缓慢，而且经过平台期之后仍然很缓慢。

优化的方法还是指数加权移动平均法（处理历史数据问题，越近的数据越有用）
![[Pasted image 20220508153736.png]]
上图是优化后的样子。

# RMSprop
![[Pasted image 20220508153756.png]]

 # Adam = RMSprop + 动量法
 ![[Pasted image 20220508154321.png]]

也许你真的很厉害呢。

# 凸优化
关于凸优化的方法要好好学一学哦。

# Reference
- [“随机梯度下降、牛顿法、动量法、Nesterov、AdaGrad、RMSprop、Adam”，打包理解对梯度下降法的优化](https://www.bilibili.com/video/BV1r64y1s7fU?spm_id_from=333.337.search-card.all.click)：这个视频讲的真的不错哦。

